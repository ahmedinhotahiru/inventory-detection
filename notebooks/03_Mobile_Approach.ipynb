{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inventory Detection System - Mobile Approach\n",
    "\n",
    "This notebook demonstrates creating a TensorFlow Lite model for detecting and counting Pepsi and Kilimanjaro products on mobile devices.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Problem**: Manual counting of inventory takes hours each week and is often skipped, causing errors.\n",
    "\n",
    "**Solution**: We'll develop a mobile-friendly model that can run directly on Android or iOS devices.\n",
    "\n",
    "**Advantages**:\n",
    "- On-device processing (no internet required)\n",
    "- Real-time detection in the field\n",
    "- Privacy-preserving (data stays on device)\n",
    "- Works in areas with poor connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow tensorflow-hub numpy pillow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display, FileLink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check for GPU Availability\n",
    "\n",
    "While not strictly necessary for this approach, GPU acceleration can speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if TensorFlow can see a GPU\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(f\"✅ GPU is available for TensorFlow\")\n",
    "    for gpu in tf.config.list_physical_devices('GPU'):\n",
    "        print(f\"  - {gpu}\")\n",
    "else:\n",
    "    print(\"⚠️ GPU is not available for TensorFlow. Using CPU instead. Training may be slower.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. View Sample Images\n",
    "\n",
    "Let's first look at our sample images to understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images\n",
    "sample_dir = \"data/sample_images\"\n",
    "sample_images = [f for f in os.listdir(sample_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, image_file in enumerate(sample_images):\n",
    "    image_path = os.path.join(sample_dir, image_file)\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(image_file)\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Training\n",
    "\n",
    "We'll organize our sample images into a structure suitable for training with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sample_dir, batch_size=32):\n",
    "    \"\"\"Prepare data for training.\"\"\"\n",
    "    print(f\"Preparing data from {sample_dir}...\")\n",
    "    \n",
    "    # Create a temporary directory for organizing data\n",
    "    temp_dir = \"temp_data\"\n",
    "    train_dir = os.path.join(temp_dir, \"train\")\n",
    "    val_dir = os.path.join(temp_dir, \"val\")\n",
    "    \n",
    "    # Create class directories\n",
    "    for class_name in [\"pepsi\", \"kilimanjaro\"]:\n",
    "        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
    "    \n",
    "    # Copy sample images to class directories\n",
    "    for file_name in os.listdir(sample_dir):\n",
    "        if not file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            continue\n",
    "            \n",
    "        if \"pepsi\" in file_name.lower():\n",
    "            class_name = \"pepsi\"\n",
    "        elif \"kilimanjaro\" in file_name.lower():\n",
    "            class_name = \"kilimanjaro\"\n",
    "        else:\n",
    "            print(f\"Skipping unknown product: {file_name}\")\n",
    "            continue\n",
    "        \n",
    "        source_path = os.path.join(sample_dir, file_name)\n",
    "        \n",
    "        # Split files between train and val (80/20)\n",
    "        if np.random.rand() < 0.8:\n",
    "            dest_path = os.path.join(train_dir, class_name, file_name)\n",
    "        else:\n",
    "            dest_path = os.path.join(val_dir, class_name, file_name)\n",
    "        \n",
    "        shutil.copy(source_path, dest_path)\n",
    "    \n",
    "    # Data augmentation for training\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    # Only rescaling for validation\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # Create generators\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Data preparation completed!\")\n",
    "    \n",
    "    # Display class mapping\n",
    "    print(f\"Class indices: {train_generator.class_indices}\")\n",
    "    \n",
    "    return train_generator, validation_generator, train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "batch_size = 8  # Small batch size since we have few samples\n",
    "train_generator, validation_generator, class_indices = prepare_data(sample_dir, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create MobileNetV2 Model\n",
    "\n",
    "MobileNetV2 is a lightweight model designed for mobile and edge devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape=(224, 224, 3), num_classes=2):\n",
    "    \"\"\"Create a MobileNetV2 model for classification.\"\"\"\n",
    "    print(\"Creating MobileNetV2 model...\")\n",
    "    \n",
    "    # Load base model\n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Add custom classification layers\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"✅ MobileNetV2 model created successfully!\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_model(num_classes=len(class_indices))\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "Now we'll train our model on the prepared data. Since we have very few samples, we'll use data augmentation and transfer learning to help prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_generator, validation_generator, epochs=10):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    print(f\"Training model for {epochs} epochs...\")\n",
    "    \n",
    "    # Create a TensorBoard callback for visualization\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Model training completed!\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "model, history = train_model(model, train_generator, validation_generator, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Keras Model\n",
    "\n",
    "Let's save our trained model before converting it to TensorFlow Lite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = \"outputs/mobile\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save Keras model\n",
    "keras_model_path = os.path.join(output_dir, \"model.h5\")\n",
    "model.save(keras_model_path)\n",
    "print(f\"✅ Keras model saved to {keras_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Convert to TensorFlow Lite\n",
    "\n",
    "Now we'll convert our Keras model to TensorFlow Lite format, which is optimized for mobile and embedded devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite(model, output_path, quantize=True):\n",
    "    \"\"\"Convert model to TensorFlow Lite format.\"\"\"\n",
    "    print(f\"Converting model to TensorFlow Lite format...\")\n",
    "    \n",
    "    # Create TFLite converter\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    # Apply optimization if requested\n",
    "    if quantize:\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        print(\"Applying quantization to reduce model size...\")\n",
    "    \n",
    "    # Convert model\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Save model\n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    print(f\"✅ TensorFlow Lite model saved to {output_path}\")\n",
    "    print(f\"  - Original model size: {os.path.getsize(keras_model_path) / (1024*1024):.2f} MB\")\n",
    "    print(f\"  - TFLite model size: {os.path.getsize(output_path) / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Convert to TFLite with quantization\n",
    "tflite_model_path = os.path.join(output_dir, \"model.tflite\")\n",
    "convert_to_tflite(model, tflite_model_path, quantize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Model Metadata\n",
    "\n",
    "To make the model easier to use on mobile devices, let's create a metadata file with important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_metadata(class_indices, output_path):\n",
    "    \"\"\"Create metadata for the TFLite model.\"\"\"\n",
    "    # Invert class indices to get index-to-label mapping\n",
    "    index_to_label = {v: k for k, v in class_indices.items()}\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = {\n",
    "        \"model_type\": \"classification\",\n",
    "        \"input_shape\": [224, 224, 3],\n",
    "        \"labels\": index_to_label,\n",
    "        \"preprocessing\": {\n",
    "            \"rescale\": \"1./255\",\n",
    "            \"mean\": [0.485, 0.456, 0.406],\n",
    "            \"std\": [0.229, 0.224, 0.225]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save metadata\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Model metadata saved to {output_path}\")\n",
    "\n",
    "# Create metadata\n",
    "metadata_path = os.path.join(output_dir, \"metadata.json\")\n",
    "create_model_metadata(class_indices, metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test the TFLite Model\n",
    "\n",
    "Let's test our TFLite model on an inventory image to make sure it works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tflite_model(model_path):\n",
    "    \"\"\"Load a TensorFlow Lite model.\"\"\"\n",
    "    # Load TFLite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    return interpreter\n",
    "\n",
    "def preprocess_image(image_path, input_shape):\n",
    "    \"\"\"Preprocess an image for model input.\"\"\"\n",
    "    # Load and resize image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image.resize((input_shape[1], input_shape[0]))\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    image_array = np.array(image, dtype=np.float32)\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    image_array = image_array / 255.0\n",
    "    image_array = (image_array - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    # Add batch dimension\n",
    "    image_array = np.expand_dims(image_array, axis=0)\n",
    "    \n",
    "    return image_array, image\n",
    "\n",
    "def detect_product(interpreter, metadata, image_path):\n",
    "    \"\"\"Detect a product in an image using the TFLite model.\"\"\"\n",
    "    # Get input and output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Get input shape\n",
    "    input_shape = input_details[0]['shape'][1:3]\n",
    "    \n",
    "    # Preprocess image\n",
    "    input_data, original_image = preprocess_image(image_path, input_shape)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(original_image)\n",
    "    plt.axis('off')\n",
    "    plt.title('Input Image')\n",
    "    plt.show()\n",
    "    \n",
    "    # Set input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    \n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get output tensor\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    # Get predicted class\n",
    "    predicted_class_index = np.argmax(output_data[0])\n",
    "    # Load metadata\n",
    "    with open(metadata, 'r') as f:\n",
    "        metadata_dict = json.load(f)\n",
    "    \n",
    "    predicted_class = metadata_dict['labels'][str(predicted_class_index)]\n",
    "    confidence = output_data[0][predicted_class_index]\n",
    "    \n",
    "    # Create result\n",
    "    pepsi_count = 1 if predicted_class == \"pepsi\" else 0\n",
    "    kilimanjaro_count = 1 if predicted_class == \"kilimanjaro\" else 0\n",
    "    \n",
    "    result = {\n",
    "        \"pepsi_count\": pepsi_count,\n",
    "        \"kilimanjaro_count\": kilimanjaro_count,\n",
    "        \"predicted_class\": predicted_class,\n",
    "        \"confidence\": float(confidence)\n",
    "    }\n",
    "    \n",
    "    return result, original_image\n",
    "\n",
    "def visualize_result(result, image):\n",
    "    \"\"\"Visualize detection result.\"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Add result as text\n",
    "    plt.figtext(0.5, 0.01, f\"Predicted: {result['predicted_class']} (Confidence: {result['confidence']:.2f})\", \n",
    "               ha=\"center\", fontsize=14, \n",
    "               bbox={\"facecolor\":\"white\", \"alpha\":0.8, \"pad\":5})\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TFLite model\n",
    "interpreter = load_tflite_model(tflite_model_path)\n",
    "\n",
    "# Test on sample images\n",
    "for image_file in sample_images:\n",
    "    image_path = os.path.join(sample_dir, image_file)\n",
    "    result, image = detect_product(interpreter, metadata_path, image_path)\n",
    "    \n",
    "    print(f\"Result for {image_file}:\")\n",
    "    print(f\"  Predicted class: {result['predicted_class']}\")\n",
    "    print(f\"  Confidence: {result['confidence']:.2f}\")\n",
    "    \n",
    "    visualize_result(result, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. A Note on Real-World Mobile Implementation\n",
    "\n",
    "For a real mobile app, we would need to:\n",
    "\n",
    "1. **Implement object detection**: The current model only classifies whole images. For actual shelf images, we would need either:\n",
    "   - Object detection model (like YOLO, but converted to TFLite)\n",
    "   - Sliding window approach with our classifier\n",
    "\n",
    "2. **Create Android/iOS application**: Build a mobile app that:\n",
    "   - Takes photos or loads images from gallery\n",
    "   - Runs the TFLite model on these images\n",
    "   - Displays results with counts and visualizations\n",
    "\n",
    "Let's create a pseudocode example of how a mobile implementation would work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```java\n",
    "// Android pseudocode for product detection\n",
    "public class ProductDetector {\n",
    "    private Interpreter tfliteInterpreter;\n",
    "    private Map<Integer, String> labelMap;\n",
    "    \n",
    "    public ProductDetector(Context context) {\n",
    "        // Load model from assets\n",
    "        tfliteInterpreter = new Interpreter(loadModelFile(context, \"model.tflite\"));\n",
    "        \n",
    "        // Load metadata\n",
    "        labelMap = loadMetadata(context, \"metadata.json\");\n",
    "    }\n",
    "    \n",
    "    public ProductResult analyzeImage(Bitmap image) {\n",
    "        // Preprocess image\n",
    "        ByteBuffer inputBuffer = preprocess(image);\n",
    "        \n",
    "        // Prepare output buffer\n",
    "        float[][] outputBuffer = new float[1][2];  // 2 classes\n",
    "        \n",
    "        // Run inference\n",
    "        tfliteInterpreter.run(inputBuffer, outputBuffer);\n",
    "        \n",
    "        // Process results\n",
    "        int pepsiCount = 0;\n",
    "        int kilimanjaroCount = 0;\n",
    "        \n",
    "        // In a real app, we would run object detection and count instances\n",
    "        // Here we're just doing basic classification\n",
    "        float[] scores = outputBuffer[0];\n",
    "        int classIndex = argmax(scores);\n",
    "        String className = labelMap.get(classIndex);\n",
    "        \n",
    "        if (\"pepsi\".equals(className)) {\n",
    "            pepsiCount = 1;\n",
    "        } else if (\"kilimanjaro\".equals(className)) {\n",
    "            kilimanjaroCount = 1;\n",
    "        }\n",
    "        \n",
    "        return new ProductResult(pepsiCount, kilimanjaroCount, className, scores[classIndex]);\n",
    "    }\n",
    "    \n",
    "    // Helper methods would be implemented here\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Mobile App Framework\n",
    "\n",
    "Finally, let's generate a skeleton for the Android app that would use our TFLite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_android_project(model_path, metadata_path, output_dir):\n",
    "    \"\"\"Create a basic Android project structure.\"\"\"\n",
    "    print(f\"Creating Android project structure in {output_dir}...\")\n",
    "    \n",
    "    # Create project directories\n",
    "    project_dir = os.path.join(output_dir, \"InventoryDetector\")\n",
    "    assets_dir = os.path.join(project_dir, \"app/src/main/assets\")\n",
    "    java_dir = os.path.join(project_dir, \"app/src/main/java/com/example/inventorydetector\")\n",
    "    \n",
    "    os.makedirs(assets_dir, exist_ok=True)\n",
    "    os.makedirs(java_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy model and metadata to assets directory\n",
    "    shutil.copy(model_path, os.path.join(assets_dir, \"model.tflite\"))\n",
    "    shutil.copy(metadata_path, os.path.join(assets_dir, \"metadata.json\"))\n",
    "    \n",
    "    # Write Java files and resources (code would be too long to include here)\n",
    "    # We would write the MainActivity.java, ModelExecutor.java, etc.\n",
    "    \n",
    "    print(f\"✅ Android project structure created at {project_dir}\")\n",
    "    print(\"To build the Android app, you would need to open this project in Android Studio.\")\n",
    "    \n",
    "    return project_dir\n",
    "\n",
    "# Create Android project structure\n",
    "android_dir = os.path.join(output_dir, \"android\")\n",
    "project_dir = create_android_project(tflite_model_path, metadata_path, android_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Conclusion\n",
    "\n",
    "### Advantages of the Mobile Approach:\n",
    "- **On-device processing**: Works without internet connection\n",
    "- **Privacy-preserving**: Data stays on the device\n",
    "- **Real-time performance**: Optimized for mobile hardware\n",
    "- **Accessibility**: Works in remote locations with poor connectivity\n",
    "\n",
    "### Limitations:\n",
    "- **Limited model size**: Mobile devices have constraints on model complexity\n",
    "- **Reduced accuracy**: Quantized models may have slightly lower accuracy\n",
    "- **Development complexity**: Requires mobile app development skills\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
